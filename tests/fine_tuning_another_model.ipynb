{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66c36756",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "# import torch.nn.functional as F\n",
    "# from torch.utils.data import Dataset, DataLoader -> datasets 라이브러리랑 충돌"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54cd86f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "1\n",
      "NVIDIA GeForce RTX 3050\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.device_count()) \n",
    "print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de28eb40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install torch==2.5.1+cu121 --index-url https://download.pytorch.org/whl/cu121\n",
    "# poetry run pip install torch==2.5.1+cu121 --index-url https://download.pytorch.org/whl/cu121"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "825d6343",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"C:/Users/ehddl/Desktop/업무/code/sns-categorizer/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "167892c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>media_cn_cleaned</th>\n",
       "      <th>label_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>휴가 돌려죠</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>관종들 릴스 릴스타그램 릴스초보</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>날이 좋아서</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>행복했던 9월 고마워</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>협찬 동결건조야채블럭 1개로 13종의 보라야채와 유산균 섭취가능 보라색 안토시아닌이...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39925</th>\n",
       "      <td>아빠들의 일사분란함은 가족애 였다 어제 캠핑하는데 전혀 예상치 못했던 비바람 돌풍이...</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39926</th>\n",
       "      <td>광고 식용유 없이 전을 굽는다고 풀무원 철판수제전 3종세트 철판 바삭감자채전 철판 ...</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39927</th>\n",
       "      <td>제품제공 이걸 소개할 수 있어 영광입니다 가장 카고스럽게 보다 더 대담하게 카고컨테...</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39928</th>\n",
       "      <td>제품제공 이제 남편들이 요리 다하겠습니다 아내님들 남편에게 식스볼트 듀라박스만 사주...</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39929</th>\n",
       "      <td>광고 튼튼함을 의심해서 죄송합니다 와이 캐리어 뭐지 새거 받자마자 3m 이상에서 떨...</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>39863 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        media_cn_cleaned  label_id\n",
       "0                                                 휴가 돌려죠        19\n",
       "1                                      관종들 릴스 릴스타그램 릴스초보        21\n",
       "2                                                 날이 좋아서        19\n",
       "3                                            행복했던 9월 고마워        19\n",
       "4      협찬 동결건조야채블럭 1개로 13종의 보라야채와 유산균 섭취가능 보라색 안토시아닌이...         8\n",
       "...                                                  ...       ...\n",
       "39925  아빠들의 일사분란함은 가족애 였다 어제 캠핑하는데 전혀 예상치 못했던 비바람 돌풍이...        25\n",
       "39926  광고 식용유 없이 전을 굽는다고 풀무원 철판수제전 3종세트 철판 바삭감자채전 철판 ...        25\n",
       "39927  제품제공 이걸 소개할 수 있어 영광입니다 가장 카고스럽게 보다 더 대담하게 카고컨테...        25\n",
       "39928  제품제공 이제 남편들이 요리 다하겠습니다 아내님들 남편에게 식스볼트 듀라박스만 사주...        25\n",
       "39929  광고 튼튼함을 의심해서 죄송합니다 와이 캐리어 뭐지 새거 받자마자 3m 이상에서 떨...        17\n",
       "\n",
       "[39863 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"tests/final_fine_tuning_data.csv\", index_col=0)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4cd22dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(data, test_size= 0.2, stratify=data['label_id'], random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868e111d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model & toknizer loading\n",
    "\n",
    "'''\n",
    "kykim/bert-kor-base : BERT-base 70GB 한국어 대용량 말뭉치 \n",
    "snunlp/KR-Medium : KR-BERT의 medium 버전, 한국어 위키 + 뉴스 + 특허 + 댓글 포함 total 12.37GB -> SNS 데이터 매우 적합\n",
    "BM-K/KoSimCSE-roberta : -> SNS 데이터 매우 적합\n",
    "distilbert-base-multilingual-cased : SNS 데이터 적합 -> 실제 활용 논문이 있긴 함\n",
    "beomi/KcELECTRA-base-v2022 : electra-base\n",
    "'''\n",
    "model_name = \"beomi/KcELECTRA-base-v2022\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# tokenizer = BertTokenizerFast.from_pretrained(model_name)\n",
    "\n",
    "## 모델이 bert 계열이고, 속도가 중요하면 BertTkenizerFast\n",
    "## 모델을 자주 비꾸거나 여러 모델 실험할 계획이면 AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "08123c9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 31890/31890 [00:03<00:00, 10111.04 examples/s]\n",
      "Map: 100%|██████████| 7973/7973 [00:00<00:00, 9756.63 examples/s] \n"
     ]
    }
   ],
   "source": [
    "# huggingface의 datasets.Dataset을 사용하는 방식\n",
    "\n",
    "dataset = DatasetDict({\n",
    "    'train' : Dataset.from_pandas(train),\n",
    "    'test' : Dataset.from_pandas(test)\n",
    "})\n",
    "\n",
    "# preprocessing\n",
    "\n",
    "def tokenize_fn(ex):\n",
    "    return tokenizer(ex[\"media_cn_cleaned\"], padding=\"max_length\", truncation=True, max_length=128)\n",
    "\n",
    "dataset = dataset.map(tokenize_fn, batched=True)\n",
    "dataset = dataset.rename_column(\"label_id\", \"label\")\n",
    "dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7c0d82d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['media_cn_cleaned', 'label', '__index_level_0__', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "    num_rows: 31890\n",
       "})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9f864582",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at beomi/KcELECTRA-base-v2022 and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# setting model\n",
    "\n",
    "num_labels = data['label_id'].nunique()\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=num_labels,\n",
    "    use_safetensors=True # gpu 버전 사용 시 추가\n",
    ")\n",
    "\n",
    "# torch gpu버전을 pip install torch==2.5.1+cu121 --index-url https://download.pytorch.org/whl/cu121로 설치\n",
    "# huggingface transformer 라이브러리가 pytorch 2.6 미만 버전은 보안 이슈로 강제로 차단하는데, gpu 버전의 torch는 현재 pip로 설치가 불가능. 따라서 우회하는 user_safetensor를 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "43801909",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting trainer\n",
    "# !pip install accelerate>=0.26.0\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"finetune-KR-Medium\", # 변경\n",
    "    eval_strategy=\"epoch\", # evaluation_strategy -> eval_strategy\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=2e-5,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=100,\n",
    "    save_total_limit=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cc640b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(p):\n",
    "    preds = p.predictions.argmax(axis=-1)\n",
    "    labels = p.label_ids\n",
    "    return {\"accuracy\": accuracy_score(labels, preds)}\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"test\"],\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9e3a4619",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5982' max='5982' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5982/5982 45:20, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.167000</td>\n",
       "      <td>1.142860</td>\n",
       "      <td>0.683933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.917400</td>\n",
       "      <td>1.088929</td>\n",
       "      <td>0.698859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.695900</td>\n",
       "      <td>1.101118</td>\n",
       "      <td>0.701367</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=5982, training_loss=1.0130163139820259, metrics={'train_runtime': 2720.5843, 'train_samples_per_second': 35.165, 'train_steps_per_second': 2.199, 'total_flos': 6294314713052160.0, 'train_loss': 1.0130163139820259, 'epoch': 3.0})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8bf44039",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting trainer\n",
    "# !pip install accelerate>=0.26.0\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"finetune-BM-K/KoSimCSE-roberta\",\n",
    "    eval_strategy=\"epoch\", # evaluation_strategy -> eval_strategy\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=2e-5,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=100,\n",
    "    save_total_limit=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "45dca887",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(p):\n",
    "    preds = p.predictions.argmax(axis=-1)\n",
    "    labels = p.label_ids\n",
    "    return {\"accuracy\": accuracy_score(labels, preds)}\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"test\"],\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "69d5c07a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5982' max='5982' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5982/5982 46:42, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.144900</td>\n",
       "      <td>1.108449</td>\n",
       "      <td>0.689577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.919000</td>\n",
       "      <td>1.051125</td>\n",
       "      <td>0.711777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.677200</td>\n",
       "      <td>1.065571</td>\n",
       "      <td>0.713282</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=5982, training_loss=1.0025360377804964, metrics={'train_runtime': 2802.7899, 'train_samples_per_second': 34.134, 'train_steps_per_second': 2.134, 'total_flos': 6294314713052160.0, 'train_loss': 1.0025360377804964, 'epoch': 3.0})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4b6cdd1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting trainer\n",
    "# !pip install accelerate>=0.26.0\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"distilbert-base-multilingual-cased\",\n",
    "    eval_strategy=\"epoch\", # evaluation_strategy -> eval_strategy\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=2e-5,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=100,\n",
    "    save_total_limit=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9295f775",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(p):\n",
    "    preds = p.predictions.argmax(axis=-1)\n",
    "    labels = p.label_ids\n",
    "    return {\"accuracy\": accuracy_score(labels, preds)}\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"test\"],\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fea6d195",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5982' max='5982' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5982/5982 46:48, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.617300</td>\n",
       "      <td>1.149738</td>\n",
       "      <td>0.688950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.487700</td>\n",
       "      <td>1.258047</td>\n",
       "      <td>0.697228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.364600</td>\n",
       "      <td>1.355011</td>\n",
       "      <td>0.701743</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=5982, training_loss=0.49658566643585134, metrics={'train_runtime': 2809.2434, 'train_samples_per_second': 34.055, 'train_steps_per_second': 2.129, 'total_flos': 6294314713052160.0, 'train_loss': 0.49658566643585134, 'epoch': 3.0})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "be12aee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting trainer\n",
    "# !pip install accelerate>=0.26.0\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"kcELECTRA-base-v2022\",\n",
    "    eval_strategy=\"epoch\", # evaluation_strategy -> eval_strategy\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=2e-5,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=100,\n",
    "    save_total_limit=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "eefb32b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(p):\n",
    "    preds = p.predictions.argmax(axis=-1)\n",
    "    labels = p.label_ids\n",
    "    return {\"accuracy\": accuracy_score(labels, preds)}\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"test\"],\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b346a16f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5982' max='5982' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5982/5982 48:56, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.233700</td>\n",
       "      <td>1.209960</td>\n",
       "      <td>0.671015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.040700</td>\n",
       "      <td>1.135771</td>\n",
       "      <td>0.694218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.843000</td>\n",
       "      <td>1.125889</td>\n",
       "      <td>0.700364</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=5982, training_loss=1.1494783550072418, metrics={'train_runtime': 2937.1075, 'train_samples_per_second': 32.573, 'train_steps_per_second': 2.037, 'total_flos': 6294314713052160.0, 'train_loss': 1.1494783550072418, 'epoch': 3.0})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed74a1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting trainer\n",
    "# !pip install accelerate>=0.26.0\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"skt-kobert-base-v1\",\n",
    "    eval_strategy=\"epoch\", # evaluation_strategy -> eval_strategy\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=2e-5,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=100,\n",
    "    save_total_limit=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81af2ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(p):\n",
    "    preds = p.predictions.argmax(axis=-1)\n",
    "    labels = p.label_ids\n",
    "    return {\"accuracy\": accuracy_score(labels, preds)}\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"test\"],\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd76157",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5982' max='5982' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5982/5982 48:56, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.233700</td>\n",
       "      <td>1.209960</td>\n",
       "      <td>0.671015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.040700</td>\n",
       "      <td>1.135771</td>\n",
       "      <td>0.694218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.843000</td>\n",
       "      <td>1.125889</td>\n",
       "      <td>0.700364</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=5982, training_loss=1.1494783550072418, metrics={'train_runtime': 2937.1075, 'train_samples_per_second': 32.573, 'train_steps_per_second': 2.037, 'total_flos': 6294314713052160.0, 'train_loss': 1.1494783550072418, 'epoch': 3.0})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a52872",
   "metadata": {},
   "source": [
    "Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69557889",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델과 토크나이저 로드\n",
    "model_path = \"src/bert_classification_model\"  # 저장된 모델 디렉토리\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "\n",
    "# 모델을 평가 모드로 전환\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e4547b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_batch(text_list):\n",
    "    inputs = tokenizer(text_list, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        preds = torch.argmax(outputs.logits, dim=1)\n",
    "    return preds.tolist()\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def predict_from_csv(csv_path):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    \n",
    "    # 학습할 때와 같은 컬럼명을 유지\n",
    "    text_list = df['media_cn_cleaned'].astype(str).tolist()\n",
    "    \n",
    "    preds = predict_batch(text_list)\n",
    "    \n",
    "    # 결과를 원래 df에 붙여서 반환\n",
    "    df['predicted_label'] = preds\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903b508c",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = predict_from_csv(\"test.csv\")\n",
    "result_df.to_csv(\"predicted_test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25506b5d",
   "metadata": {},
   "source": [
    "##### Using Torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823d0c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # huggingface가 아니라 pytorch library 사용 시 방법\n",
    "# from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# class TokenDataset(Dataset):\n",
    "  \n",
    "#     def __init__(self, dataframe, model_name):\n",
    "#         # sentence, label 컬럼으로 구성된 데이터프레임 전달\n",
    "#         self.data = dataframe        \n",
    "#         # Huggingface 토크나이저 생성\n",
    "#         # self.tokenizer = BertTokenizerFast.from_pretrained(tokenizer_pretrained)\n",
    "#         self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "  \n",
    "#     def __len__(self):\n",
    "#         return len(self.data)\n",
    "  \n",
    "#     def __getitem__(self, idx):\n",
    "#         sentence = self.data.iloc[idx]['document']\n",
    "#         label = self.data.iloc[idx]['label']\n",
    "\n",
    "#         # 토큰화 처리\n",
    "#         tokens = self.tokenizer(\n",
    "#             sentence,                # 1개 문장 \n",
    "#             return_tensors='pt',     # 텐서로 반환\n",
    "#             truncation=True,         # 잘라내기 적용\n",
    "#             padding='max_length',    # 패딩 적용\n",
    "#             add_special_tokens=True  # 스페셜 토큰 적용\n",
    "#         )\n",
    "\n",
    "#         input_ids = tokens['input_ids'].squeeze(0)           # 2D -> 1D\n",
    "#         attention_mask = tokens['attention_mask'].squeeze(0) # 2D -> 1D\n",
    "#         token_type_ids = torch.zeros_like(attention_mask)\n",
    "\n",
    "#         # input_ids, attention_mask, token_type_ids 이렇게 3가지 요소를 반환하도록 합니다.\n",
    "#         # input_ids: 토큰\n",
    "#         # attention_mask: 실제 단어가 존재하면 1, 패딩이면 0 (패딩은 0이 아닐 수 있습니다)\n",
    "#         # token_type_ids: 문장을 구분하는 id. 단일 문장인 경우에는 전부 0\n",
    "#         return {\n",
    "#             'input_ids': input_ids,\n",
    "#             'attention_mask': attention_mask, \n",
    "#             'token_type_ids': token_type_ids,\n",
    "#         }, torch.tensor(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaae1057",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # train, test 데이터셋 생성\n",
    "# train_data = TokenDataset(train, model_name)\n",
    "# test_data = TokenDataset(test, model_name)\n",
    "\n",
    "# # DataLoader로 이전에 생성한 Dataset를 지정하여, batch 구성, shuffle, num_workers 등을 설정합니다.\n",
    "# train_loader = DataLoader(train_data, batch_size=8, shuffle=True, num_workers=8)\n",
    "# test_loader = DataLoader(test_data, batch_size=8, shuffle=True, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c291e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 1개의 batch 꺼내기\n",
    "# inputs, labels = next(iter(train_loader))\n",
    "\n",
    "# # 데이터셋을 device 설정\n",
    "# inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "# labels.to(device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sns-categorizer-wO1G7-CE-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
